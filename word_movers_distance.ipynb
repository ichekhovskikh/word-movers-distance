{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word-movers-distance.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN0MhTy3UQY1W5pbiAXzG8n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ichekhovskikh/word-movers-distance/blob/master/word_movers_distance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoLW8xsaGFWr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pymorphy2[fast]\n",
        "\n",
        "import gensim\n",
        "import os\n",
        "import collections\n",
        "import smart_open\n",
        "import random\n",
        "import json\n",
        "import urllib.request\n",
        "import pymorphy2\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from pymystem3 import Mystem\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.similarities import WmdSimilarity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZYD5AXcNYRU",
        "colab_type": "text"
      },
      "source": [
        "#Начинаем\n",
        "Для начала нам понадобится комплект документов для обучения нашей модели doc2vec. Теоретически, документ может быть чем угодно: коротким твитом из 140 символов, отдельным абзацем, новостной статьей или книгой. В NLP комплект документов часто называют корпусом.\n",
        "\n",
        "Будем тренировать нашу модель на собственном корпусе. Этот корпус содержит 500 научных статей на 5 различных тем.\n",
        "\n",
        "Для тестирования будет использоваться тестовый корпус из 50 статей (10 статей на каждую тему).\n",
        "\n",
        "Dataset состоит из трех строк: id (идентификатор строки), text (текст статьи), tag (идентификатор самой статьи, вектор которого будем обучать), class_name (был размечен вручную, необходим только для тестирования)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGeR-R2dGNQ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Введите путь к файлам исходной базы статей:\n",
        "train_path = 'https://raw.githubusercontent.com/ichekhovskikh/cyberleninka-article-downloader/master/train_corpus.json' #@param {type: \"string\"}\n",
        "test_path = 'https://raw.githubusercontent.com/ichekhovskikh/cyberleninka-article-downloader/master/test_corpus.json' #@param {type: \"string\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzY-WMJjJhtR",
        "colab_type": "text"
      },
      "source": [
        "## Опредлим функцию для чтения и предварительной обработки текста\n",
        "Ниже мы определяем функцию для открытия  train/test файла, предварительно обрабатываем каждый текст датасета, используя простой инструмент предварительной обработки gensim (то есть, разбиваем текст на отдельные слова, удалите знаки препинания, установите строчные буквы и т. д.), лемматизацию, удаление стоп слов и возвращаем список слов. Для обучения модели нам нужно будет связать тег с каждым документом учебного корпуса. В нашем случае тег - это идентификатор статьи."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbwpMLNmwZ4v",
        "colab_type": "text"
      },
      "source": [
        "Лемматизация каждого слова статьи:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFO_znw317kM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "def lemmatize(words):\n",
        "    for word in words:\n",
        "        yield morph.parse(word)[0].normal_form"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hr6Qz8Ucw_ee",
        "colab_type": "text"
      },
      "source": [
        "Удаление стоп слов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRm8tmSe17Hq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "russian_stopwords = stopwords.words(\"russian\")\n",
        "\n",
        "def remove_stopwords(words):\n",
        "    return [word for word in words if word not in russian_stopwords]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aB82kEYi-nek",
        "colab_type": "text"
      },
      "source": [
        "Предобработка текста статьи:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHwqCiuN-hDS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def advanced_preprocess(text):\n",
        "    normalized_text = gensim.utils.simple_preprocess(text)\n",
        "    normalized_text = list(lemmatize(normalized_text))\n",
        "    normalized_text = remove_stopwords(normalized_text)\n",
        "    return normalized_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5agV69PVxEIl",
        "colab_type": "text"
      },
      "source": [
        "Отрытие файла с корпусом статей:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMO4zHrqGTws",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_corpus(corpus_path):\n",
        "    with urllib.request.urlopen(corpus_path) as corpus_url:\n",
        "        corpus = json.loads(corpus_url.read().decode())\n",
        "        return [advanced_preprocess(article['text']) for article in corpus]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ui3oWTnrxKNt",
        "colab_type": "text"
      },
      "source": [
        "Получение исходного текста статьи по индексу:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1gSBZ1G1BNj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_article_text_by_index(index, corpus_path):\n",
        "    with urllib.request.urlopen(corpus_path) as corpus_url:\n",
        "        corpus = json.loads(corpus_url.read().decode())\n",
        "        return corpus[index]['text']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsDE0caoRrMN",
        "colab_type": "text"
      },
      "source": [
        "Получение категории статьи по индексу:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhMV7VB-Rn-Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_article_class_by_index(index, corpus_path):\n",
        "    with urllib.request.urlopen(corpus_path) as corpus_url:\n",
        "        corpus = json.loads(corpus_url.read().decode())\n",
        "        return corpus[index]['class_name']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7oXB7nLRuUh",
        "colab_type": "text"
      },
      "source": [
        "Загружаем корпуса:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0vJrTipGemC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_corpus = read_corpus(train_path)\n",
        "test_corpus = read_corpus(test_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKA3sFQxKYJP",
        "colab_type": "text"
      },
      "source": [
        "Давайте посмотрим на учебный корпус:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9aPeDPvGgve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_corpus[:2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29upY6eyKlmk",
        "colab_type": "text"
      },
      "source": [
        "Корпус тестирования выглядит так:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrWLt9Z8Gm66",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(test_corpus[:2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luOlm4Q5K2jj",
        "colab_type": "text"
      },
      "source": [
        "# Обучение модели\n",
        "## Создание объекта WmdSimilarity\n",
        "Теперь мы создадим модель Woc2Vec с векторным размером 120 слов и перебираем учебный корпус 500 раз (данные параметры были получены после проведения ряда исследований).\n",
        "\n",
        "Словарь содержит в себе все уникальные слова, извлеченных из учебного корпуса.\n",
        "Параметры обучения:\n",
        "- sentences: список текстов\n",
        "- size: размерность вектора\n",
        "- window: максимальное расстояние между текущим и прогнозируемым словом в предложении\n",
        "- sg: алгоритм обучения (1 - skip-gram, 0 - CBOW)\n",
        "- alpha: коэфициент обучения\n",
        "- iter: количество эпох."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6w0x7RlnzVD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Укажите параметры обучения модели:\n",
        "vector_size = 120 #@param\n",
        "window =  6 #@param\n",
        "epochs =  500 #@param\n",
        "alpha = 0.001 #@param\n",
        "learning_method = \"skip-gram\" #@param [\"skip-gram\", \"CBOW\"] {type:\"string\"}\n",
        "\n",
        "sg = 1\n",
        "if (learning_method == \"CBOW\"):\n",
        "    sg = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ur-INzRGxqE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%time model = Word2Vec(train_corpus, size=vector_size, window=window, alpha=alpha, iter=epochs, sg=sg, workers=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwIh-b55Pn-_",
        "colab_type": "text"
      },
      "source": [
        "Теперь будем использовать обученный Word2Vec для нахождения близости тектов на основе Word Mover's Distance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80bB-Yz1Pm7Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wmd_similarity = WmdSimilarity(train_corpus, model, num_best=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRfMjEATNm-y",
        "colab_type": "text"
      },
      "source": [
        "# Оценочная модель\n",
        "Чтобы оценить нашу новую модель, мы сначала выведем новые векторы для каждого документа тренировочного корпуса, сравним выведенные векторы с тренировочным корпусом.\n",
        "\n",
        "Проверка выведенного вектора по обучающему вектору является своего рода «проверкой работоспособности» в отношении того, ведет ли модель себя адекватно, хотя и не является реальным значением «точности».\n",
        "\n",
        "Можем взглянуть на пример:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gTpgGnFHgH0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc_id = random.randint(0, len(train_corpus) - 1)\n",
        "doc = train_corpus[doc_id]\n",
        "sims = wmd_similarity[doc]\n",
        "print('ТЕКСТ ИСХДНОГО ДОКУМЕНТА ({}): «{}»\\n'.format(doc_id, get_article_text_by_index(doc_id, train_path)))\n",
        "num = 0\n",
        "for index, similarity in sims:\n",
        "    num += 1\n",
        "    print(u'%s) %s: «%s»\\n' % (num, similarity, get_article_text_by_index(index, train_path)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eq8rXTHOwBQ",
        "colab_type": "text"
      },
      "source": [
        "# Тестирование модели\n",
        "Используя тот же подход, что и выше, мы выведем вектор для случайно выбранного тестового документа и сравним документ с нашей моделью на глаз."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5yDhL0JHy9v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "test_size = len(test_corpus)\n",
        "\n",
        "predicted_classes = [] \n",
        "test_classes = []\n",
        "\n",
        "for doc_index in range(test_size):\n",
        "    doc = test_corpus[doc_index]\n",
        "    sims = wmd_similarity[doc]\n",
        "    \n",
        "    test_article_class = get_article_class_by_index(doc_index, test_path)\n",
        "    print('ТЕКСТ ИСХДНОГО ДОКУМЕНТА ({}) «{}»: «{}»\\n'.format(doc_index, test_article_class, get_article_text_by_index(doc_index, test_path)))\n",
        "    num = 0\n",
        "    for index, similarity in sims:\n",
        "        num += 1\n",
        "        predicted_article_class = get_article_class_by_index(index, train_path)\n",
        "        print(u'%s) %s «%s»: «%s»\\n' % (num, similarity, predicted_article_class, get_article_text_by_index(index, train_path)))\n",
        "        predicted_classes.append(predicted_article_class)\n",
        "        test_classes.append(test_article_class)\n",
        "        \n",
        "print('f1score: {}'.format(f1_score(test_classes, predicted_classes, average='macro')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91sOWCLiLOxS",
        "colab_type": "text"
      },
      "source": [
        "Тестирование на малых данных для выявления ошибок в ходе написания кода"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIgFPnpL6CBe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.test.utils import common_texts\n",
        "\n",
        "text = advanced_preprocess(\"test interface minors\")\n",
        "\n",
        "print('common texts = ', common_texts)\n",
        "print('text = ', text)\n",
        "\n",
        "model = Word2Vec(common_texts, size=300, window=5, min_count=1, iter=10, workers=4)\n",
        "wmd_similarity = WmdSimilarity(common_texts, model, num_best=10)\n",
        "\n",
        "\n",
        "sims = wmd_similarity[text]\n",
        "num = 0\n",
        "for index, similarity in sims:\n",
        "    num += 1\n",
        "    print(u'%s) %s: «%s»\\n' % (num, similarity, common_texts[index]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BSpkRxWUrC9B"
      },
      "source": [
        "# Поиск похожих научных документов\n",
        "Выполните поиск похожих научных статей"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrflInTzrgmW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Укажите путь к тексту статьи в формате *.txt или введите текст статьи:\n",
        "article_text = '' #@param {type: \"string\"}\n",
        "article_path = 'https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/PY0101EN/labs/example1.txt' #@param {type: \"string\"}\n",
        "\n",
        "if (article_text == ''):\n",
        "    with urllib.request.urlopen(article_path) as article_url:\n",
        "      article_text = article_url.read().decode()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6JqjIFpu46v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "normalized_text = advanced_preprocess(article_text)\n",
        "sims = wmd_similarity[normalized_text]\n",
        "\n",
        "print('ТЕКСТ ИСХДНОГО ДОКУМЕНТА «{}»\\n'.format(article_text)\n",
        "num = 0\n",
        "for index, similarity in sims:\n",
        "    num += 1\n",
        "    print(u'%s) %s: «%s»\\n' % (num, similarity, get_article_text_by_index(index, train_path)))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}